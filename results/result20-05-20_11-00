nohup: les entrées sont ignorées
20_05_20_11-00
Starting script
Using backend: pytorch
##################################################
# Statistics file : dataset_50-50_1inst_50nod_40cap_1dep_50000iter_0.8decay_0.35destr_18determ.pickle
# Hidden node dimensions : [32, 32]
# Hidden edge dimensions : [32, 16]
# Hidden linear dimension : 32
# Dropout probability : 0.2
# Max epoch : 5000
# Initial learning rate : 1e-05
##################################################
1 Retrieved alns statistics
2 Created new cvrp state
Using device cuda
3 Creating inputs and labels ... created inputs, and created labels
4 Created GCN
50.0% of null iterations
Dataset size : 2552
Training set size : 1871
5 Starting training...
Epoch 0, loss 0.166270, accuracy 0.0694
Epoch 5, loss 0.176300, accuracy 0.0694
Epoch 10, loss 0.187754, accuracy 0.0694
Epoch 15, loss 0.200301, accuracy 0.0694
Epoch 20, loss 0.213116, accuracy 0.0717
Epoch 25, loss 0.225162, accuracy 0.1246
Epoch 30, loss 0.235902, accuracy 0.1246
Epoch 35, loss 0.244933, accuracy 0.1246
Epoch 40, loss 0.252188, accuracy 0.1246
Epoch 45, loss 0.258036, accuracy 0.1246
Epoch 50, loss 0.262789, accuracy 0.1246
Epoch 55, loss 0.266543, accuracy 0.1246
Epoch 60, loss 0.269399, accuracy 0.1246
Epoch 65, loss 0.271559, accuracy 0.1246
Epoch 70, loss 0.273215, accuracy 0.1246
Epoch 75, loss 0.274498, accuracy 0.1246
Epoch 80, loss 0.275484, accuracy 0.1246
Epoch 85, loss 0.276219, accuracy 0.1246
Epoch 90, loss 0.276748, accuracy 0.1246
Epoch 95, loss 0.277124, accuracy 0.1246
Epoch 100, loss 0.277396, accuracy 0.1246
Epoch 105, loss 0.277595, accuracy 0.1246
Epoch 110, loss 0.277743, accuracy 0.1246
Epoch 115, loss 0.277855, accuracy 0.1246
Epoch 120, loss 0.277940, accuracy 0.1246
Epoch 125, loss 0.278007, accuracy 0.1246
Epoch 130, loss 0.278060, accuracy 0.1246
Epoch 135, loss 0.278103, accuracy 0.1246
Epoch 140, loss 0.278139, accuracy 0.1246
Epoch 145, loss 0.278168, accuracy 0.1246
Epoch 150, loss 0.278194, accuracy 0.1246
Epoch 155, loss 0.278216, accuracy 0.1246
Epoch 160, loss 0.278235, accuracy 0.1246
Epoch 165, loss 0.278252, accuracy 0.1246
Epoch 170, loss 0.278266, accuracy 0.1246
Epoch 175, loss 0.278279, accuracy 0.1246
Epoch 180, loss 0.278291, accuracy 0.1246
Epoch 185, loss 0.278301, accuracy 0.1246
Epoch 190, loss 0.278310, accuracy 0.1246
Epoch 195, loss 0.278318, accuracy 0.1246
Epoch 200, loss 0.278325, accuracy 0.1246
Epoch 205, loss 0.278332, accuracy 0.1246
Epoch 210, loss 0.278338, accuracy 0.1246
Epoch 215, loss 0.278343, accuracy 0.1246
Epoch 220, loss 0.278348, accuracy 0.1246
Epoch 225, loss 0.278353, accuracy 0.1246
Epoch 230, loss 0.278358, accuracy 0.1246
Epoch 235, loss 0.278362, accuracy 0.1246
Epoch 240, loss 0.278366, accuracy 0.1246
Epoch 245, loss 0.278370, accuracy 0.1246
Epoch 250, loss 0.278373, accuracy 0.1246
Epoch 255, loss 0.278377, accuracy 0.1246
Epoch 260, loss 0.278381, accuracy 0.1246
Epoch 265, loss 0.278384, accuracy 0.1246
Epoch 270, loss 0.278387, accuracy 0.1246
Epoch 275, loss 0.278390, accuracy 0.1246
Epoch 280, loss 0.278393, accuracy 0.1246
Epoch 285, loss 0.278396, accuracy 0.1246
Epoch 290, loss 0.278399, accuracy 0.1246
Epoch 295, loss 0.278401, accuracy 0.1246
Epoch 300, loss 0.278404, accuracy 0.1246
Epoch 305, loss 0.278407, accuracy 0.1246
Epoch 310, loss 0.278410, accuracy 0.1246
Epoch 315, loss 0.278412, accuracy 0.1246
Epoch 320, loss 0.278415, accuracy 0.1246
Epoch 325, loss 0.278417, accuracy 0.1246
Epoch 330, loss 0.278420, accuracy 0.1246
Epoch 335, loss 0.278422, accuracy 0.1246
Epoch 340, loss 0.278425, accuracy 0.1246
Epoch 345, loss 0.278428, accuracy 0.1246
Epoch 350, loss 0.278431, accuracy 0.1246
Epoch 355, loss 0.278434, accuracy 0.1246
Epoch 360, loss 0.278436, accuracy 0.1246
Epoch 365, loss 0.278439, accuracy 0.1246
Epoch 370, loss 0.278442, accuracy 0.1246
Epoch 375, loss 0.278444, accuracy 0.1246
Epoch 380, loss 0.278447, accuracy 0.1246
Epoch 385, loss 0.278450, accuracy 0.1246
Epoch 390, loss 0.278452, accuracy 0.1246
Epoch 395, loss 0.278454, accuracy 0.1246
Epoch 400, loss 0.278456, accuracy 0.1246
Epoch 405, loss 0.278459, accuracy 0.1246
Epoch 410, loss 0.278461, accuracy 0.1246
Epoch 415, loss 0.278463, accuracy 0.1246
Epoch 420, loss 0.278465, accuracy 0.1246
Epoch 425, loss 0.278467, accuracy 0.1246
Epoch 430, loss 0.278469, accuracy 0.1246
Epoch 435, loss 0.278471, accuracy 0.1246
Epoch 440, loss 0.278473, accuracy 0.1246
Epoch 445, loss 0.278475, accuracy 0.1246
Epoch 450, loss 0.278477, accuracy 0.1246
Epoch 455, loss 0.278479, accuracy 0.1246
Epoch 460, loss 0.278480, accuracy 0.1246
Epoch 465, loss 0.278482, accuracy 0.1246
Epoch 470, loss 0.278485, accuracy 0.1246
Epoch 475, loss 0.278487, accuracy 0.1246
Epoch 480, loss 0.278489, accuracy 0.1246
Epoch 485, loss 0.278491, accuracy 0.1246
Epoch 490, loss 0.278493, accuracy 0.1246
Epoch 495, loss 0.278495, accuracy 0.1246
Epoch 500, loss 0.278497, accuracy 0.1246
Epoch 505, loss 0.278499, accuracy 0.1246
Epoch 510, loss 0.278501, accuracy 0.1246
Epoch 515, loss 0.278504, accuracy 0.1246
Epoch 520, loss 0.278506, accuracy 0.1246
Epoch 525, loss 0.278508, accuracy 0.1246
Epoch 530, loss 0.278510, accuracy 0.1246
Epoch 535, loss 0.278513, accuracy 0.1246
Epoch 540, loss 0.278514, accuracy 0.1246
Epoch 545, loss 0.278517, accuracy 0.1246
Epoch 550, loss 0.278519, accuracy 0.1246
Epoch 555, loss 0.278521, accuracy 0.1246
Epoch 560, loss 0.278523, accuracy 0.1246
Epoch 565, loss 0.278525, accuracy 0.1246
Epoch 570, loss 0.278527, accuracy 0.1246
Epoch 575, loss 0.278529, accuracy 0.1246
Epoch 580, loss 0.278531, accuracy 0.1246
Epoch 585, loss 0.278534, accuracy 0.1246
Epoch 590, loss 0.278535, accuracy 0.1246
Epoch 595, loss 0.278538, accuracy 0.1246
Epoch 600, loss 0.278540, accuracy 0.1246
Epoch 605, loss 0.278542, accuracy 0.1246
Epoch 610, loss 0.278544, accuracy 0.1246
Epoch 615, loss 0.278545, accuracy 0.1246
Epoch 620, loss 0.278547, accuracy 0.1246
Epoch 625, loss 0.278549, accuracy 0.1246
Epoch 630, loss 0.278551, accuracy 0.1246
Epoch 635, loss 0.278553, accuracy 0.1246
Epoch 640, loss 0.278555, accuracy 0.1246
Epoch 645, loss 0.278557, accuracy 0.1246
Epoch 650, loss 0.278559, accuracy 0.1246
Epoch 655, loss 0.278561, accuracy 0.1246
Epoch 660, loss 0.278562, accuracy 0.1246
Epoch 665, loss 0.278564, accuracy 0.1246
Epoch 670, loss 0.278566, accuracy 0.1246
Epoch 675, loss 0.278568, accuracy 0.1246
Epoch 680, loss 0.278570, accuracy 0.1246
Epoch 685, loss 0.278571, accuracy 0.1246
Epoch 690, loss 0.278573, accuracy 0.1246
Epoch 695, loss 0.278575, accuracy 0.1246
Epoch 700, loss 0.278577, accuracy 0.1246
Epoch 705, loss 0.278578, accuracy 0.1246
Epoch 710, loss 0.278580, accuracy 0.1246
Epoch 715, loss 0.278582, accuracy 0.1246
Epoch 720, loss 0.278584, accuracy 0.1246
